\documentclass[twoside,11pt]{article}
\usepackage{jmlr2e}
%\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue,breaklinks=true]{hyperref}
\usepackage{url}
\jmlrheading{1}{2011}{XX}{XX/11}{XXX}{Pedregosa, Varoquaux, Gramfort et al.}

% Short headings should be running head and authors last names

\usepackage{amsmath}
\usepackage{xcolor}
% For highlighting changes
\usepackage{soul}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\ShortHeadings{scikits.learn: machine learning in Python}{Pedregosa, Varoquaux, Gramfort et al.}
\firstpageno{1}


\begin{document}

\title{scikits.learn: Machine learning in Python}


\author{\name Fabian Pedregosa \email fabian.pedregosa@inria.fr \\
        \name Ga\"el Varoquaux \email gael.varoquaux@normalesup.org  \\
        \name Alexandre Gramfort \email alexandre.gramfort@inria.fr \\
        \name Vincent Michel  \email vincent.michel@logila.fr \\
        \name Bertrand Thirion  \email bertrand.thirion@inria.fr \\
        \addr Parietal, INRIA Saclay / Neurospin,
      B\^at 145, CEA Saclay, 91191 Gif sur Yvette -- {\sc France}
        \AND
        \name Olivier Grisel \email olivier.grisel@ensta.fr \\
        \addr Nuxeo, 20 rue Soleillet, 75 020 Paris -- {\sc France} 
        \AND
        \name Mathieu Blondel \email mblondel@ai.cs.kobe-u.ac.jp \\
        \addr Kobe University, 1-1 Rokkodai, Nada, Kobe 657-8501 -- {\sc Japan}
        \AND
        \name Peter Prettenhofer \email peter.prettenhofer@gmail.com \\
        \addr Bauhaus-Universit\"at Weimar, Bauhausstr. 11, 99421 Weimar -- {\sc Germany}
        \AND
        \name Ron Weiss \email ronweiss@gmail.com \\
        \addr Music and Audio Research Lab, New York University, New York -- {\sc USA}
        \AND
        \name Vincent Dubourg \email vincent.dubourg@gmail.com\\
        \addr Clermont Universit\'e, IFMA, EA 3867, LaMI,
        BP 10448, 63000 Clermont-Ferrand -- {\sc France}
        \AND
        \name Jake Vanderplas \email vanderplas@astro.washington.edu\\
        \addr Survey Science Group, Dept. Astronomy, University of Washington, Seattle -- {\sc USA}
  \AND
        \name Alexandre Passos \email alexandre.tp@gmail.com \\
        \addr RECOD Lab,  Unicamp,  Campinas, S\~ao Paulo -- {\sc Brazil}
        \AND
        \name David Cournapeau \email cournape@gmail.com \\
        \addr Kyoto -- {\sc Japan}
        \AND
        \name Matthieu Brucher \email matthieu.brucher@gmail.com \\
        \addr Total E\&P France,  Pau -- {\sc France}
        \AND
        \name Matthieu Perrot \email matthieu.perrot@cea.fr\\
        \name \'Edouard Duchesnay \email edouard.duchesnay@cea.fr \\
        \addr LNAO / Neurospin,
      B\^at 145, CEA Saclay, 91191 Gif sur Yvette -- {\sc France}
}


\editor{?}

\maketitle

\begin{abstract}
%
\emph{Scikits.learn} is a Python module integrating a wide range of
state-of-the-art machine learning algorithms for medium-scale supervised
and unsupervised problems. This package focuses on bringing machine
learning to non-specialists using a general-purpose high-level language.
Emphasis is put on ease of use, performance, documentation, and API
consistency.

%
It has minimal dependencies and is distributed under the simplified BSD
license, encouraging its use in both academic and commercial settings.
Source code, binaries, and documentation can be downloaded from
\url{http://scikit-learn.sourceforge.net}.

\end{abstract}

\keywords{Python, supervised learning, unsupervised learning, model
selection}


%%  It offers a wide range of
%% methods such as Support Vector Machines, linear models (L1, L2
%% penalized), logistic regression, gaussian mixture models and more.


\section{Introduction}

The Python programming language is establishing itself as one of the
most popular languages for scientific computing. Thanks to its
high-level interactive nature and its maturing ecosystem of scientific
libraries, it is an appealling choice for algorithmic development and
exploratory data analysis \citep{cise2007,cise2011}. Yet, as a
general-purpose language, it is increasingly employed not only in academic
settings but also in industry.

{\sl Scikits.learn} harnesses this rich environment to provide state-of-the-art
implementations of
many well known machine learning algorithms, while maintaining an
easy-to-use interface tightly integrated with the Python language. This answers the
growing need for statistical data analysis by non-specialists in the software and web
industries, as well as in fields outside of computer-science, such as biology or physics.
\emph{Scikits.learn} differs from other machine learning toolboxes in Python for
various reasons: \emph{i)} it is distributed under the BSD license
\emph{ii)} it incorporates compiled code for efficiency, unlike MDP
\citep{zito2008} and pybrain \citep{schaul2010}, \emph{iii)} it depends
only on numpy and scipy to facilitate easy distribution, 
unlike pymvpa \citep{hanke2009} \hl{we avoid wrappers to R code}, and \emph{iv)} it focuses on imperative
programming, unlike pybrain which uses a data-flow framework
for building artificial neural networks.

While the package is mostly written in Python, it incorporates the C++
libraries LibSVM \citep{chang2001} and LibLinear \citep{fan2008} that
provide reference implementations of SVMs and generalized linear models
with compatible licenses.
%
Binary packages are available on a rich set of platforms including
Windows and any POSIX platforms. Furthermore, thanks to its liberal
license, it has been widely distributed as part
of major free software distributions such as Ubuntu, Debian, Mandriva,
NetBSD and Macports and in commercial distributions such as the ``Enthought
Python Distribution''.

%%The library is structured in submodules, each of which covers a family
%%of related algorithms. This way, the svm module support vector
%%machine-related algorithms of classification (SVC), regression (SVR)
%%and unsupervised learning (OneClassSVM).
%%
%%% Mention 'flat is better than nested'

%%% Mention the quality of our SVM bindings


\section {Project vision}

\noindent{\bf Code quality}
%
Rather than providing as many features as possible, the project's goal has been to provide solid
implementations. The quality of the code base is ensured with unit
%% RONWEISS: I've never head the term "unitary test" before :)
tests: as of release \hl{0.8, test coverage is 81\%} and the use of static
analysis tools such as {\tt pyflakes} and {\tt pep8}. Finally, we
strive to use consistent naming for the various functions and
parameters used throughout a strict adherence to the Python coding
guidelines and numpy style documentation.

\smallskip \noindent{\bf BSD licensing}
%
Most of the Python ecosystem is licensed with non-copyleft licenses such
as the BSD license. While this licensing scheme is beneficial for adoption
of these tools by commercial
projects, it does impose some restrictions: we are unable to use some existing
scientific code, such as the GSL (GNU Scientific Library).

\smallskip \noindent{\bf Bare-bone design and API}
%
To lower the barrier of entry, we avoid framework code and keep the number
of different objects to a minimum.
The package is built
around the numpy array \citep{Vanderwalt2011}, which is common to the
scientific Python ecosystem.

\smallskip
\noindent{\bf Community-driven development}
%
We base our development on collaborative tools such as git, github and
public mailing lists. External contributions are welcome and
encouraged via developer manual and API guidelines.

\smallskip \noindent{\bf Documentation}
%
We believe that documentation is a key component of software.
\emph{Scikits.learn} provides a $\sim$300 page user guide including
narrative documentation, class references, a tutorial, installation
instructions, as well as more than 60 examples, some of which feature
real-world applications. We try to minimize the use of
machine-learning jargon, while maintaining precision with
regards to the algorithms employed.


\section{Underlying technologies}

%\emph{Scikits.learn} builds upon a powerful stack of existing libraries:

%XXX: Shorten the above

\noindent{\bf Numpy}:
%
the base data structure used for data and model parameters.
The data for supervised or unsupervised learning is
presented as one, or a few, numpy arrays, thus integrating seamlessly
with other scientific Python libraries. Numpy's view-based memory
model limits copies, even when binding with compiled code. It also
provides basic arithmetic operations.
%This choice strives for easy of use by using plain numpy
%arrays instead of specialized data structures and limiting framework
%code. Our objects work directly with NumPy arrays, providing a
%flexible way to communicate with other packages.

\smallskip
\noindent{\bf Scipy}:
%
%a package built on Numpy which contains
%
efficient algorithms for linear algebra, sparse matrix representation,
special functions and basic statistical functions. {\sl Scipy} has
bindings for many Fortran-based standard numerical packages, such as
LAPACK. This is important for ease of installation and portability, as
providing libraries around Fortran code can prove challenging on
various platforms.

\smallskip
\noindent{\bf Cython}:
%
a language for writing C extensions for Python. Cython makes it easy to
reach the performance of compiled languages while retaining
Python-like syntax and high-level operations. It is also used to
easily bind compiled libraries, eliminating most of the boilerplate code
associated with Python/C extensions.

%\smallskip
%\noindent{\bf Matplotlib}:
%%
%visualization and plotting. Matplotlib is used only in the examples.

\section{Code design}

\noindent{\bf Objects specified by interface, not by inheritance}
%
In order to facilitate the use of external objects with \emph{scikits.learn},
inheritance is not enforced; instead, code
conventions provide a consistent interface.
The central object is an {\tt estimator}, that implements a
{\tt fit} method, accepting as arguments an input data array and,
optionally, an array of labels for supervised problems. Supervised estimators,
such as SVM classifiers, can implement a {\tt predict} method. Some estimators,
that we call {\tt transformers}, \emph{e.g.} PCA, implement a {\tt
transform} method, returning modified input data.
%
Estimators may also provide a {\tt score} method, which evaluate its
goodness of fit. It must increase with the goodness of fit and
may correspond to a log-likelihood, or a negated loss function.
%
The other important object is the \emph{cross-validation iterator},
which provides pairs of train and test indices to split input
data accordingly. It may be a standard list, or a generator
which dynamically
creates these indices.
Examples comprise K-fold,
leave one out, or
stratified cross-validation.


\smallskip \noindent{\bf Model selection}
%
\emph{Scikits.learn} can evaluate an estimator's performance or select
parameters using cross-validation, optionally distributing the
computation to several cores.
This is accomplished by wrapping an estimator in a {\tt GridSearchCV}
object, where the ``CV'' stands for ``cross-validated''.
During the call to {\tt fit}, it selects the parameters
on a specified parameter grid, maximizing a score function
(the {\tt score} method of the underlying estimator). The calls to {\tt
predict}, {\tt score}, or {\tt transform} are then delegated to the tuned
estimator. This object can therefore be used transparently in the same
way as any other
estimator.
Such cross validation can be made more efficient for certain
estimators by exploiting their specific properties, such as warm restarts
or regularization paths \citep{friedman2010}. This is supported through special
%cross-validated
objects, such as the {\tt LassoCV}.
%
Finally, a {\tt Pipeline} object can
combine several {\tt transformers} and a final estimator to create a
combined estimator to \emph{e.g.} apply dimension reduction before
fitting. {\tt Pipeline} behave as a standard estimator, and can
therefore be used
with the {\tt GridSearchCV} to tune the parameters of all steps together.

\section{High-level yet efficient: some trade offs}

%Special care has been take on algorithmic efficiency, producing
%algorithms that are ofter faster than the ones found in compiled
%libraries.

While \emph{scikits.learn} focuses on ease of use, and is
mostly written in a high level language, care has been taken to maximize
computational efficiency. In table \ref{tab:comparisons}, we compare
computation time for a few algorithms implemented in the major machine
learning toolkits accessible in Python. We use the Madelon data
set \citep{Guyon2004}, 4400 instances and 500 attributes,
that can be used in supervised
and unsupervised settings.  The data set is quite large, but small enough for most
algorithms to run.
%
% RONWEISS: It would be great to figure out a way to include this
% description of the dataset in the text.
%
% which was part of the NIPS 2003 feature selection challenge. It is an
% artificial dataset
%containing data points grouped in 32 clusters placed on the vertices of
%a five dimensional hypercube and randomly labeled +1 or -1.
%
%
In the following, we discuss these results and illustrate
the engineering trade-offs related to performance.

%{{{----------------------------------------------------------------------------
\begin{table}[t]
\small
\hspace*{.03\linewidth}%
%\begin{minipage}{1.04\linewidth}
\begin{tabular}{l c c c c c c}
\hline\hline %inserts double horizontal lines
 & scikits.learn & mlpy & pybrain & pymvpa &  mdp & shogun \\ [0.5ex]
\hline
Support Vector Classification & {\bf 5.2} & 9.47 & 17.5 & 11.52 & 40.48 & 5.63 \\
Lasso (LARS) & {\bf 1.17} & 105.3   & - &  37.35 & - & - \\
Elastic Net & {\bf 0.52} & 73.7 & -  &  1.44  & -  & - \\
k-Nearest Neighbors & 0.57 & 1.41 & - &  {\bf 0.56} & 0.58 & 1.36 \\
PCA (9 components) & {\bf 0.18} & - & - & 8.93  & 0.47 & 0.33 \\
k-Means (9 clusters) & 1.34 &  0.79 & $\star$ & -  & 35.75 & {\bf 0.68} \\
License &  BSD & GPL & BSD  &  BSD  & BSD  & GPL \\
\hline
\end{tabular}

-: Not implemented. \hfill
$\star$: Does not converge within 1 hour.

\vspace*{-1.5ex}
%\end{minipage}
\caption{\small
Time in seconds on the Madelon dataset for various machine learning libraries exposed in Python:
MLPy \citep{albanese2008}, PyBrain \citep{schaul2010}, pymvpa
\citep{hanke2009}, MDP \citep{zito2008} and Shogun
\citep{sonnenburg2010}. 
\hl{More benchmarks as well as their code}, can be retrieved from {\tt http://github.com/scikit-learn}.
\vspace*{-1.5em}\label{tab:comparisons}
}
\end{table}
%----------------------------------------------------------------------------}}}

\smallskip \noindent{\bf SVM}:
%
While all of the packages compared call libsvm in the
background, the
performance of \emph{scikits.learn} can be explained by two factors.
First, our bindings have up to 40\% less overhead than the original
libsvm Python bindings: they use Cython's numpy support
to avoid memory copies. Second, we patch libsvm to improve efficiency
on dense data, use a smaller memory footprint, and better utilize memory
alignment and pipelining capabilities of modern processors. This patched
version also provides unique features, such as working without copies on
sparse data and setting weights for individual samples.


\smallskip \noindent{\bf LARS}:
%
Iteratively refining the residuals instead of recomputing them gives
performance gains of 2--10 times over the reference R implementation
\citep{LARS}. {\sl Pymvpa} uses this implementation via the Rpy R
bindings and pays a heavy price to memory copies.


\smallskip \noindent{\bf Elastic Net}:
%
We benchmarked coordinate descent implementations of Elastic Net.  In
\emph{scikits.learn}, it is coded in Cython and uses low-level BLAS
routines for fast array operations. It achieves the same order of
performance as the highly optimized Fortran version \emph{glmnet}
\citep{friedman2010} on medium-scale problems, but performance on very
large problems is limited since we do not use the KKT conditions to
define an active set.

\smallskip
\noindent{\bf kNN}:
%
The k-nearest neighbors classifer implementation constructs a ball
tree \citep{omohundro1989} of the samples, but defaults to a more
efficient brute force search in large dimensions.

\smallskip \noindent{\bf PCA}:
%
For medium to large datasets, \emph{scikits.learn} provides an
implementation of a truncated PCA based on random projections
\citep{rokhlin2009}. On medium-scale problems, incomplete eigenvalue
decomposition, as used by MDP, also performs very well.

\smallskip
\noindent{\bf k-means}:
%
\emph{scikits.learn}'s k-means algorithm is implemented in pure
Python.  Its performance is limited by the fact that numpy's
array operations take multiple passes over data.

\section{Conclusion}

\emph{Scikits.learn} exposes a wide variety of machine learning
algorithms, both supervised and unsupervised, using a consistent,
task-oriented interface, thus enabling easy comparison of methods for a
given application.
%
Since it relies on the scientific Python ecosystem, it can easily be
integrated into applications outside the traditional range of statistical
data analysis. Importantly, the algorithms, implemented in a high-level
language, can be used as building blocks for approaches dedicated to
specific use cases, \emph{e.g.} in medical imaging \citep{Michel2011}.
%
Future work includes the \emph{online} learning, to scale to
large datasets.

\bibliography{scikit}

\end{document}
