\documentclass[twoside,11pt]{article} 
\usepackage{jmlr2e} 
%\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue,breaklinks=true]{hyperref}
\usepackage{url}
\jmlrheading{1}{2011}{XX}{XX/11}{XXX}{Pedregosa, Varoquaux, Gramfort et al.}

% Short headings should be running head and authors last names

\usepackage{xcolor}
\usepackage[normalem]{ulem}
\usepackage{amsmath}

\newcommand{\GAEL}[1]{\textcolor{blue}{\uline{#1}}}
\newcommand{\FABIAN}[1]{\textcolor{red}{\uline{#1}}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\ShortHeadings{scikits.learn: machine learning in Python}{Pedregosa, Varoquaux, Gramfort et al.}
\firstpageno{1}


\begin{document}

\title{scikits.learn: machine learning in Python}


\author{\name Fabian Pedregosa \email fabian.pedregosa@inria.fr \\
        \name Ga\"el Varoquaux \email gael.varoquaux@normalesup.org  \\
        \name Alexandre Gramfort \email alexandre.gramfort@inria.fr \\
        \name Vincent Michel  \email vincent.michel@inria.fr \\
        \name Bertrand Thirion  \email bertrand.thirion@inria.fr \\
        \addr Parietal, INRIA Saclay / Neurospin, 
	    B\^at 145, CEA Saclay, 91191 Gif sur Yvette -- {\sc France}
        \AND
        \name Olivier Grisel \email olivier.grisel@ensta.fr \\
        \addr Nuxeo -- {\sc France} 
        \AND
        \name Matthieu Blondel \email mathieu@mblondel.org \\
        \addr Kobe University -- {\sc Japan} 
        \AND
        \name Peter Prettenhofer \email peter.prettenhofer@gmail.com \\
        \addr Bauhaus-Universit\"at Weimar -- {\sc Germany}
        \AND
        \name Ron Weiss \email ronweiss@gmail.com \\
        \addr MARL, NYU, New York -- {\sc USA}
        \AND
        \name Vincent Dubourg \email vincent.dubourg@gmail.com\\
        \addr  -- {\sc France}
        \AND
        \name Jake Vanderplas \email vanderplas@astro.washington.edu\\
        \addr University of Washington -- {\sc USA}
	\AND
        \name Alexandre Passos \email alexandre.tp@gmail.com \\
        \addr Unicamp,  Campinas -- {\sc Brazil}
        \AND
        \name David Cournapeau \email cournape@gmail.com \\
        \addr Kyoto -- {\sc Japan}
        \AND
        \name Matthieu Brucher \email matthieu.brucher@gmail.com \\
        \addr Total E\&P,  Pau -- {\sc France}
        \AND
        \name Matthieu Perrot \email matthieu.perrot@cea.fr\\
        \name \'Edouard Duchesnay \email edouard.duchesnay@cea.fr \\
        \addr LNAO / Neurospin, 
	    B\^at 145, CEA Saclay, 91191 Gif sur Yvette -- {\sc France}
}


\editor{?}

\maketitle

\begin{abstract}
\emph{Scikits.learn} is a Python module integrating
a wide range of state-of-the-art machine learning algorithms for
medium-scale supervised and unsupervised problems. This package
focuses on bringing machine learning to non-specialists using a
general-purpose high-level language with a special care on ease of use,
documentation and a consistent API.
%
It has minimal dependencies and is licensed under the simplified BSD
license, encouraging its use in both academic and commercial settings. Source
code, binaries, and documentation can be downloaded from
\url{http://scikit-learn.sourceforge.net}.

\end{abstract}

\keywords{Python, supervised learning, unsupervised learning, model
selection}


%%  It offers a wide range of
%% methods such as Support Vector Machines, linear models (L1, L2
%% penalized), logistic regression, gaussian mixture models and more.


\section{Introduction}

The Python programming language is establishing itself as one of the
languages of choice for scientific computing. Its high-level interactive
nature and its maturing ecosystem of scientific libraries
\citep{cise2007,cise2011} make it a very appealing choice algorithmic
development and data processing in both academic and industrial settings.
In particular, as it is a general-purpose language, it enjoys success in
applied problems as well as amongst computer scientists.
%
{\sl Scikits.learn} harnesses this rich environment to provide
state-of-the-art machine learning tools to non-specialists, answering a
growing need for statistical data analysis in the software and web
industries but also for basic research in non computer-science fields,
such as biology or physics.

The goal of the project is to provide a reference implementation of some
of the best known machine learning algorithms, while maintaining an
easy-to-use interface tightly integrated in the Python language.
\emph{Scikits.learn} differs from other learning toolboxes in Python for
various reasons: \emph{i)} it is distributed under the BSD license
\emph{ii)} it incorporates compiled code for efficiency --unlike MDP
\citep{zito2008} and pybrain \citep{schaul2010}-- \emph{iii)} it depends
only on numpy and scipy --unlike pymvpa \citep{hanke2009}, we depending
on R for distribution reasons-- \emph{iv)} it focuses on imperative
programming and not data-flow frameworks --pybrain uses such a framework
for building artificial neural networks.

While the package is mostly coded in Python, it incorporates the C++
libraries LibSVM \citep{chang2001} and LibLinear \citep{fan2008} that
provide reference implementation of SVM and generalized linear models
with compatible licenses.
%
Binary packages are available on a rich set of platforms including
Windows and any POSIX platforms. Furthermore, it is distributed as part
of major open source distributions such as Ubuntu, Debian, Mandriva,
NetBSD or Macports and in commercial distributions such as ``Enthought
Python Distribution'' thanks to its liberal license.

%%The library is structured in submodules, each of which covers a family
%%of related algorithms. This way, the svm module support vector
%%machine-related algorithms of classification (SVC), regression (SVR)
%%and unsupervised learning (OneClassSVM).
%%
%%% Mention 'flat is better than nested'

%%% Mention the quality of our SVM bindings


\section {Project vision}

\noindent{\bf BSD licensing}
%
Most of the Python ecosystem is licensed with non copyleft licensed such
as the BSD license. This licensing scheme is beneficial for adoption
outside the academic world, via the use of these tools in commercial
projects. It does impose some restrictions: we cannot use some existing
scientific code, such as the GSL (Gnu Scientific Library).

\smallskip \noindent{\bf Bare-bone design and API}
%
For a low barrier of entry, we avoid framework code and keep the number
of different objects to a minimum. In particular, we use as a dataset
object the numpy array \citep{Vanderwalt2011} that is common to the
scientific Python ecosystem.

\smallskip
\noindent{\bf Community-driven development}
%
We base our development on collaborative tools such as git, github and
public mailing lists. External contributions are welcomed and
encouraged via developer manual and API guidelines.

\smallskip \noindent{\bf Code quality}
%
Rather than aim for many features, our goal is to provide solid
implementation. The quality of the code base is ensured with unitary
tests: as of release 0.6, test coverage is 78\%. We follow the Python
coding style guidelines, as well as the numpy style for documentation. We
employ static analysis tools such as {\tt pyflakes} and {\tt pep8}.
Finally, we strive to have consistent naming for the various functions
and parameters.

\smallskip \noindent{\bf Documentation}
%
We believe that documentation is a key component of software.
\emph{Scikits.learn} provides a $\sim$300 page user guide including
narrative documentation, class reference, tutorial, installation
instructions as well as a collection of more than 60 examples, some of
them featuring real-world applications. We try to minimize the amount of
technical, machine-learning specific, terms used, while maintaining
scientific precision with regards to the algorithms employed.


\section{Underlying technologies}

%\emph{Scikits.learn} builds upon a powerful stack of existing libraries:

%XXX: Shorten the above

\smallskip
\noindent{\bf Numpy}:
%
the base data structure for representing
information. The data for supervised or unsupervised learning is
presented as one, or a few, numpy arrays, thus integrating seamlessly
with other scientific Python libraries. Numpy's view-based memory 
model limits copies, even when binding with compiled code. It also 
provides basic arithmetic operations. 
%This choice strives for easy of use by using plain numpy
%arrays instead of specialized data structures and limiting framework
%code. Our objects work directly with NumPy arrays, providing a
%flexible way to communicate with other packages.

\smallskip
\noindent{\bf Scipy}:
%
efficient algorithms for linear algebra, sparse matrix structure, special
functions and basic statistical functions. {\sl Scipy} has binding for
many Fortran-based standard numerical packages. This is important for
ease of install and portability, as providing binaries binding Fortran
can prove challenging on various platforms. 

\smallskip
\noindent{\bf Cython}:
%
generation of C code seamlessly inserted in Python. Cython allows to
reach the performance of compiled languages as C while retaining
Python-like syntax and high-level operations. Also, Cython is used to
easily bind compiled libraries, eliminating most of the boilerplate code
associated with Python/C extensions.

\smallskip
\noindent{\bf Matplotlib}:
%
visualization and plotting. Matplotlib is used only in the examples.

\section{High-level yet efficient: some trade offs}

%Special care has been take on algorithmic efficiency, producing
%algorithms that are ofter faster than the ones found in compiled
%libraries.

While the focus \emph{scikits.learn} is on ease of the use, and the
code is written mostly in a high level language, care has been taken
on computational efficiency. In table \ref{tab:comparisons}, we
compare computation time for a few algorithms implemented in most of
the major machine learning toolkits accessible in Python.  


For these benchmarks, we used the Madelon \citep{Guyon2004} data set,
a 4400 instances and 500 attributes dataset which was part of the NIPS
2003 feature selection challenge. It is an artificial dataset
containing data points grouped in 32 clusters placed on the vertices
of a five dimensional hypercube and randomly labeled +1 or -1.


In the following, we discuss some aspects of these results, to
illustrate the engineer trade offs related to performance in a
high-level language. Note that, as of release 0.6, the
\emph{scikits.learn} code was not yet modified to benefit from the
lessons learned from these benchmarks.

%{{{----------------------------------------------------------------------------
\begin{table}[tb]
\small
\hspace*{.03\linewidth}%
%\begin{minipage}{1.04\linewidth}
\begin{tabular}{l c c c c c c}
\hline\hline %inserts double horizontal lines 
 & scikits.learn & mlpy & pybrain & pymvpa &  mdp & shogun \\ [0.5ex]
\hline
Support Vector Classification & 9.44 & 16.78 & 17.46 & 26.09 & 52.36 & {\bf 8.79} \\
Lasso (LARS) & {\bf 1.45} & 168.25   & -       &  64.89     & -    & - \\
Elastic Net & {\bf 0.42} & 73.06 & -  &  6.53  & -  & - \\
k-Nearest Neighbors & 1.38 & {\bf 0.30}  & - &  0.75 & 59.62    & 0.73 \\
PCA (9 components) & {\bf 0.42} & 3.01  & 2.94  & $\star^1$ & 0.58  & - \\
k-Means  & 2.64 & 0.66 & $\star$ & -  & {\bf 0.28} & 0.49 \\
License &  BSD & GPL & BSD  &  BSD  & BSD  & GPL \\
\hline
\end{tabular}

$\star$: Not converging after 1 hour iteration. \hfill $^1$: pympva does
not allow for truncated PCA.
\vspace*{-1ex}
%\end{minipage}
\caption{\small
Time in seconds for various machine learning libraries exposed in Python:
MLPy \citep{albanese2008}, PyBrain \citep{schaul2010}, pymvpa
\citep{hanke2009}, MDP \citep{zito2008} and Shogun
\citep{sonnenburg2010}. \label{tab:comparisons}}
\end{table}
%----------------------------------------------------------------------------}}}

\smallskip
\noindent{\bf SVM}:
%
While all packages compared for support vector classification use
libsvm \citep{chang2001} in the background, good performance of
\emph{scikits.learn} can be explained by two factors. First of, our
bindings have low overhead, as they are implemented in Cython and
avoid memory copies. As can be seen in the comparason, this overhead
reduction translates in a speedup of 40\% when compared to libraries
that use the python bindings as shipped by libsvm \citep{chang2001}.
Second, we've patched libsvm to work efficiency on dense data,
providing less memory footprint, better usage of memory alignment and
pipelining capabilities of modern processors. Our patched version of
libsvm also provides some unique features, such as the ability to work
natively with sparse data and to set weights for individual samples.


\smallskip
\noindent{\bf LARS}:
By iteratively refining the residuals and instead of recomputing them,
we are able to get performance gains of 2x to 10x over the reference R
implementation \citep{LARS}. The {\sl pymvpa} package uses this
implementation via the Rpy R bindings and thus pays a heavy price to
memory copies.


\smallskip
\noindent{\bf ElasticNet}:
%
We benchmarked coordinate descent implementations of elasticnet.  In
\emph{scikits.learn}, it is coded in Cython and uses low-level BLAS
routines for fast array multiplication. \FABIAN{Not sure wether this
  belongs here }In case BLAS is not found on the host system,
\emph{scikits.learn} will compile its own unoptimized version of these
routines.  It achieves the same order of performance as the highly
optimized Fortran version \emph{glmnet} \citep{friedman2010} on
medium-scale problems but performance on very large problems will be
limited as we do not use the KKT conditions to define an active set.

\smallskip
\noindent{\bf kNN}:
%
The k-nearest neighbors is achieved by constructing a ball
tree \citep{omohundro1989} of the samples. This strategy is very
efficient in large dimensions, but is not favorable for low-dimensional
problems. \GAEL{This does not explain why we do not perform poorly}

\smallskip
\noindent{\bf PCA}:
%
For medium to large datasets, \emph{scikits.learn} provides an
implementation of a truncated PCA based on random projections
\citep{rokhlin2009}. On medium-scale problems, the NIPALS algorithm, used
by MDP also performs very well.

\smallskip
\noindent{\bf k-Means}:
%
Other pure-Python packages outperform the k-Means implementation of
\emph{scikits.learn} as they take advantage of the triangular inequality
\citep{elkan2003}.

\section{Code design}

\paragraph{Code by interface, not by inheritance}

Inheritance is not enforced at any stage, instead, we adopt some
simple conventions that make the API easier to learn. We call an
estimator to any object that implements that implement a `fit` and
`predict` method, independently of its class hierarchy. Examples of
estimators include Support Vector Classification, k-Nearest Neighbors
and the Lasso.

Some more specialized objects might also implement a score function
for testing the accuracy of an estimator. Some classes of unsupervised
also implement a transform for [...].


%% However, some properties in the algorithms (warm restarts, path
%% solutions, etc.) can, and should be used be used to speed up the
%% process. To allow this The methods are model-dependent and are
%% implemented in classes that append ``CV'', for cross-validation, to
%% the model name (Lasso and LassoCV, etc.)


\paragraph {Model selection}

Model selection is the problem of finding optimal parameters for a
model with the data at hand [...]. Usually, this process is carried
out by measuring the performance of the different classifiers over all
possible parameters.

To ease this repetitive task, \emph{scikits.learn} proposes some convenience
objects. GridSearchCV is takes as input a classifier and a list of
parameters and stores the best one. This works because all estimators
in \emph{scikits.learn} implement functions .\emph{fit}, \emph{predict} and
possibly \emph{score} so GridSearchCV is able to use classifiers as
black boxes. For convenience, GridSearchCV is also a classifier
(implements fit and predict), thus can be used transparently as any
other classifier.

However, some models allow for further optimizations. For example,
LARS provides the full path, coordinate descent methods allow for warm
restarts, Ridge has a closed form for leave-one-out cross validation,
etc. In these cases, we provide optimized cross-validating
classifiers. In these cases, we append ``CV'' to the name of the
classifier.



\section{Conclusions}

Today the project already contains more than 40 algorithms ranging
from supervised to unsupervised learning, providing an unequal
framework for easily comparison of different methods on a given
application.

While the choice of Python ensures code reuse in the package, it also
binds us to this language and limits its use outside the Python
language.

This effectively rules out code sharing with other major scientific
platforms, such as MATLAB\texttrademark, Octave, and R.



\bibliography{scikit}

\end{document}
